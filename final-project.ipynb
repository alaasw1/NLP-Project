{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aabc2fc",
   "metadata": {},
   "source": [
    "# Final Project - Alaa Sweed - 318462959"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8ba3b3",
   "metadata": {},
   "source": [
    "# Final Project:Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3bfb1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7005bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open and read the text file\n",
    "with open(\"Restaurant_Reviews.tsv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    #skip first column header line\n",
    "    next(file)\n",
    "    #read the reset of the data in file\n",
    "    restaurant_reviews = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45c8b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after data cleaning and preprocessing we save new review in restaurant_reviews_updated list\n",
    "restaurant_reviews_updated = []\n",
    "#now we iterrate in each review and clean it and lemmatize it \n",
    "import re\n",
    "for review in restaurant_reviews:\n",
    "    [review,liked]= review.strip().split('\\t') \n",
    "    #remove special characters from the text except of english chars or nubmer or \\n\n",
    "    review = re.sub('[^a-zA-Z0-9\\n]', ' ', review)\n",
    "    #convert the text to lower case\n",
    "    review = review.lower()\n",
    "    #make doc object from the review\n",
    "    doc = nlp(review)\n",
    "    #filter out stop words and punctuation after lemmatization\n",
    "    lemma_review = [token.lemma_ for token in doc if token.lemma_ not in nlp.Defaults.stop_words]\n",
    "    #convert lemma_review list back to string\n",
    "    lemma_review_str = \" \".join(lemma_review)\n",
    "    #filter out null or empty reviews and liked values after cleaning reviews and lemmatization\n",
    "    if lemma_review_str.strip() == '' or liked.strip() == '' or lemma_review_str is None or liked is None:\n",
    "        continue\n",
    "    #append the updated review and liked value to the list in .tsv format\n",
    "    restaurant_reviews_updated.append(f\"{lemma_review_str}\\t{liked}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc04e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the updated restaurant reviews to a new .tsv file (Restaurant_Reviews_Updated.tsv)\n",
    "with open(\"Restaurant_Reviews_Updated.tsv\", \"w\", encoding=\"utf-8\") as file:\n",
    "    #first we write column headers\n",
    "    file.write(\"Review\\tLiked\\n\")\n",
    "    #then we write all reviews data\n",
    "    file.writelines(restaurant_reviews_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe0255f",
   "metadata": {},
   "source": [
    "# Final Project:Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "80022480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#load the new updated dataset into a pandas datafram (Restaurant_Reviews_Updated.tsv)\n",
    "#because we have .tsv file with 'Review' and 'Liked' columns ,they are seprated by tap in tsv format\n",
    "df = pd.read_csv('Restaurant_Reviews_Updated.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "729d1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['Review']\n",
    "y= df['Liked']\n",
    "#split the data into train & test sets : train 0.67 and test 0.33 and we do it randomly!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77fbc57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#define result table list for all the algorethims\n",
    "results_table=[]\n",
    "#Decision Tree algorethim\n",
    "text_clf_DT=Pipeline([('tfidf', TfidfVectorizer()), ('DTclf', DecisionTreeClassifier())])\n",
    "#feed the training data through the pipeline\n",
    "text_clf_DT.fit(X_train, y_train)\n",
    "#get a prediction set\n",
    "predictions = text_clf_DT.predict(X_test)\n",
    "#get result of Decision Tree algorethim\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions, average='weighted')\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "f1 = f1_score(y_test, predictions, average='weighted')\n",
    "#add result of Decision Tree algorethim to result table list\n",
    "results_table.append(['Decision Tree', accuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aae784b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Random Forest algorethim\n",
    "text_clf_RF=Pipeline([('tfidf', TfidfVectorizer()), ('RFclf', RandomForestClassifier())])\n",
    "#feed the training data through the pipeline\n",
    "text_clf_RF.fit(X_train, y_train)\n",
    "#get a prediction set\n",
    "predictions = text_clf_RF.predict(X_test)\n",
    "#get result of Random Forest algorethim\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions, average='weighted')\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "f1 = f1_score(y_test, predictions, average='weighted')\n",
    "#add result of Random Forest algorethim to result table list\n",
    "results_table.append(['Random Forest', accuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2fdaf9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "#Support Vector Machines algorethim\n",
    "text_clf_SV=Pipeline([('tfidf', TfidfVectorizer()), ('SVclf', SVC())])\n",
    "#feed the training data through the pipeline\n",
    "text_clf_SV.fit(X_train, y_train)\n",
    "#get a prediction set\n",
    "predictions = text_clf_SV.predict(X_test)\n",
    "#get result of Support Vector Machines algorethim\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions, average='weighted')\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "f1 = f1_score(y_test, predictions, average='weighted')\n",
    "#add result of Support Vector Machines algorethim to result table list\n",
    "results_table.append(['Support Vector Machines', accuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa9a7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#Logistic Regression algorethim\n",
    "text_clf_LR=Pipeline([('tfidf', TfidfVectorizer()), ('LRclf', LogisticRegression())])\n",
    "#feed the training data through the pipeline\n",
    "text_clf_LR.fit(X_train, y_train)\n",
    "#get a prediction set\n",
    "predictions = text_clf_LR.predict(X_test)\n",
    "#get result of Logistic Regression algorethim\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions, average='weighted')\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "f1 = f1_score(y_test, predictions, average='weighted')\n",
    "#add result of Logistic Regression algorethim to result table list\n",
    "results_table.append(['Logistic Regression', accuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e372321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#Gradient Boosting algorethim\n",
    "text_clf_GB=Pipeline([('tfidf', TfidfVectorizer()), ('GBclf', GradientBoostingClassifier())])\n",
    "#feed the training data through the pipeline\n",
    "text_clf_GB.fit(X_train, y_train)\n",
    "#get a prediction set\n",
    "predictions = text_clf_GB.predict(X_test)\n",
    "#get result of Gradient Boosting algorethim\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions, average='weighted')\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "f1 = f1_score(y_test, predictions, average='weighted')\n",
    "#add result of Gradient Boosting algorethim to result table\n",
    "results_table.append(['Gradient Boosting', accuracy, precision, recall, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a9958690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Algorithm  Accuracy  Precision    Recall  F1 Score\n",
      "0            Decision Tree  0.704268   0.705379  0.704268  0.704293\n",
      "1            Random Forest  0.728659   0.737460  0.728659  0.727264\n",
      "2  Support Vector Machines  0.780488   0.783111  0.780488  0.780374\n",
      "3      Logistic Regression  0.774390   0.776366  0.774390  0.774340\n",
      "4        Gradient Boosting  0.762195   0.771254  0.762195  0.761078\n"
     ]
    }
   ],
   "source": [
    "#print the results in a table\n",
    "results_table_df = pd.DataFrame(results_table, columns=['Algorithm', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "print(results_table_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "16357f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the result we got, my conclusion is:\n",
    "#the Support Vector Machines (SVM) algorithm has the highest accuracy so it have the most correct predictions overall.\n",
    "#the Random Forest algorithm has the highest precision so it have the highest ability to accurately identify positive cases.\n",
    "#the Support Vector Machines (SVM) have the highest recall so it good at finding all positive cases.\n",
    "#the Gradient Boosting algorithm achieves the highest F1 score so it shows a good balance between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4788b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the results, the insights into the challenges faced are:\n",
    "#Decision Tree:it doesn't do well in terms of accuracy, precision, recall, and F1 score.\n",
    "#Random Forest:it has trouble with high accuracy and accurately classify certain instances.\n",
    "#Support Vector Machines (SVM):it's highly accurate, but there's not much room for improvement in precision and recall.\n",
    "#Logistic Regression:it's highly accurate, but there's not much room for improvement in precision and recall(but SVM is better).\n",
    "#Gradient Boosting:it achieves a good balance between precision and recall, but overall accuracy is not the highest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
